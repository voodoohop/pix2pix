#!/bin/bash
trap "exit" INT

MODEL=$1
SOURCE_VIDEO=$2
SOURCE_NAME=$(basename "$SOURCE_VIDEO")
CANNY_PARAM=${3:-"-blur 0x5 -type Grayscale -contrast-stretch 0"}
VIDEOSIZE=${4:-"512"}
CANNY_SAFE=${CANNY_PARAM//[\%]/p}s
RAWNAME="$SOURCE_NAME"_"$CANNY_SAFE"_"$MODEL"
NAME=${RAWNAME// /_}
FPS=20

echo "NAME: $NAME"
sleep 1
rm -r "/tmp/imgs/video_sequences/$NAME"
mkdir -p "/tmp/imgs/video_sequences/$NAME"
mkdir "/tmp/imgs/video_sequences/$NAME/A"
mkdir "/tmp/imgs/video_sequences/$NAME/B"
mkdir "/tmp/imgs/video_sequences/$NAME/AB"
echo "extracting video frames and audio..."
#ls -l $SOURCE_VIDEO
#ls -l uploads
ffmpeg -i "$SOURCE_VIDEO" -r $FPS -vf scale=$VIDEOSIZE:$VIDEOSIZE "/tmp/imgs/video_sequences/$NAME/A/frame_%05d.png"
ffmpeg -i "$SOURCE_VIDEO" -y -vn -c:a libmp3lame "/tmp/imgs/video_sequences/$NAME/output.mp3"


#cp imgs/video_sequences/$NAME/A/frame_*.png "/tmp/imgs/video_sequences/$NAME/B/"
echo "combining edge and target images..."
for file in /tmp/imgs/video_sequences/$NAME/A/frame_*.png
do
  filename=$(basename $file)
  echo "file $file filename $filename"
  imageA=$file
  echo "imageA $imageA"
  imageB="/tmp/imgs/video_sequences/$NAME/B/$filename"
  imageAB="/tmp/imgs/video_sequences/$NAME/AB/$filename"
  echo "combining $imageA $imageB ... and running edge detector... with $CANNY_PARAM"
  convert $imageA $CANNY_PARAM $imageB
  convert +append $imageA $imageB $imageAB
done
echo "running neural network..."
rm -r results/$MODEL/latest_net_G_AB/images/
DATA_ROOT=/tmp/imgs/video_sequences/$NAME name=$MODEL which_direction=BtoA phase=AB gpu=1 cudnn=1 loadSize=$VIDEOSIZE fineSize=$VIDEOSIZE th test.lua
echo "combining rendered frames and audio..."   
ffmpeg -y -f image2 -pattern_type glob -framerate $FPS -i "results/$MODEL/latest_net_G_AB/images/output/*.png"  -i "/tmp/imgs/video_sequences/$NAME/output.mp3" -c:v prores_ks -profile:v 3  -f mov -acodec libmp3lame -aq 2 -crf 17  -r $FPS "results/$NAME.mov"
# ffplay -loop 0 "results/$NAME.mov" &
rm -r "/tmp/imgs/video_sequences/$NAME"
